from typing import Dict, Any, List
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.document_loaders import PDFPlumberLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain.storage import InMemoryStore
from langchain.retrievers import ParentDocumentRetriever
from pathlib import Path
from .state import AnalysisState
from app.core.config import get_settings
import asyncio
import uuid


class PipelineNodes:
    def __init__(self):
        self.settings = get_settings()

        self.llm = ChatOpenAI(
            model=self.settings.openai_model,
            temperature=self.settings.openai_temperature,
            api_key=self.settings.openai_api_key,
        )

        self.embeddings = OpenAIEmbeddings(
            model=self.settings.openai_embedding_model,
            api_key=self.settings.openai_api_key,
        )

        self.parent_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.settings.parent_chunk_size,
            chunk_overlap=self.settings.parent_chunk_overlap,
            separators=["\n\n", "\n", ".", "!", "?", " "],
        )

        self.child_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.settings.child_chunk_size,
            chunk_overlap=self.settings.child_chunk_overlap,
            separators=["\n\n", "\n", ".", "!", "?", " "],
        )

        # The vectorstore to use to index the child chunks
        self.vectorstore = Chroma(
            collection_name="split_parents",
            embedding_function=self.embeddings,
            persist_directory=self.setting.chorma_db_path,
        )

        # The storage layer for the parent documents
        self.store = InMemoryStore()

        # Initialize the retriever
        self.retriever = ParentDocumentRetriever(
            vectorstore=self.vectorstore,
            docstore=self.store,
            child_splitter=self.child_splitter,
            parent_splitter=self.parent_splitter,
        )

    async def document_preprocessor(self, state: AnalysisState) -> AnalysisState:
        """PDF ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        try:
            file_path = state["file_path"]

            # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if not self._validate_file(file_path):
                state["error"] = f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"
                state["processing_status"] = "failed"
                return state

            # ë¬¸ì„œ ID ìƒì„±
            state["doc_id"] = str(uuid.uuid4())

            # PDF íŒŒì¼ ë¡œë“œ
            loader = PDFPlumberLoader(file_path)
            state["raw_docs"] = loader.load()

            state["processing_status"] = "preprocessing_completed"

            return state

        except Exception as e:
            state["error"] = str(e)
            state["processing_status"] = "failed"
            return state

    async def chunk_and_embed_document(self, state: AnalysisState) -> AnalysisState:
        """ë¬¸ì„œ ì²­í‚¹"""
        try:
            documents = state["raw_docs"]
            if not documents:
                state["error"] = f"ì›ë³¸ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
                state["processing_status"] = "failed"
                return state

            if len(documents) > 50:  # í° ë¬¸ì„œì¸ ê²½ìš°
                await self.retriever.aadd_documents(documents)
            else:
                self.retriever.add_documents(documents)

            state["processing_status"] = "chunking_completed"
            state["total_chunks"] = len(documents)

            # ë²¡í„°ìŠ¤í† ì–´ í†µê³„ ì¡°íšŒ
            vectorstore_stats = await self._get_vectorstore_stats()
            state["embedding_stats"] = {
                "total_vectors": vectorstore_stats.get("total_vectors", 0),
                "embedding_model": self.settings.openai_embedding_model,
                "collection_name": self.settings.chroma_collection_name,
            }

            return state

        except Exception as e:
            state["error"] = f"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ :{str(e)}"
            state["processing_status"] = "failed"
            return state

    async def summary_agent(self, state: AnalysisState) -> AnalysisState:
        """ë¬¸ì„œ ìš”ì•½ ìƒì„± ì—ì´ì „íŠ¸"""
        try:
            document_content = state["document_content"]
            summary_type = state.get("summary_type", "auto")

            # ìš”ì•½ íƒ€ì…ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì„ íƒ
            if summary_type == "detailed":
                prompt = self._get_detailed_summary_prompt()
            elif summary_type == "brief":
                prompt = self._get_brief_summary_prompt()
            else:
                prompt = self._get_auto_summary_prompt()

            # LLM í˜¸ì¶œ
            formatted_prompt = prompt.format(content=document_content)
            response = await self.llm.ainvoke(
                [{"role": "user", "content": formatted_prompt}]
            )

            # êµ¬ì¡°í™”ëœ ìš”ì•½ ìƒì„±
            summary = self._parse_summary_response(response.content)
            state["summary"] = summary

            return state

        except Exception as e:
            state["error"] = str(e)
            state["processing_status"] = "failed"
            return state

    async def query_agent(self, state: AnalysisState) -> AnalysisState:
        """ì§ˆì˜ì‘ë‹µ ì—ì´ì „íŠ¸"""
        try:
            query = state["query"]
            document_id = state["document_id"]

            # RAG ê²€ìƒ‰
            relevant_chunks = await self._search_relevant_chunks(document_id, query)

            # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±
            prompt = ChatPromptTemplate.from_template(
                """
            ë‹¹ì‹ ì€ ê³µì‹œ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
            
            ì§ˆë¬¸: {query}
            
            ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©:
            {context}
            
            ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
            ë‹µë³€ì€ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:
            
            ## í•µì‹¬ ë‹µë³€
            [ì£¼ìš” ë‹µë³€ ë‚´ìš©]
            
            ## ì„¸ë¶€ ë¶„ì„
            [ìƒì„¸í•œ ë¶„ì„ ë‚´ìš©]
            
            ## ê·¼ê±° ìë£Œ
            [ì°¸ì¡°í•œ ë¬¸ì„œ ë¶€ë¶„]
            """
            )

            context = "\n".join(relevant_chunks)
            formatted_prompt = prompt.format(query=query, context=context)

            response = await self.llm.ainvoke(
                [{"role": "user", "content": formatted_prompt}]
            )

            state["answer"] = response.content
            state["sources"] = relevant_chunks
            state["confidence"] = self._calculate_confidence(query, relevant_chunks)

            return state

        except Exception as e:
            state["error"] = str(e)
            state["processing_status"] = "failed"
            return state

    def route_by_step(self, state: AnalysisState) -> str:
        """ë‹¨ê³„ë³„ ë¼ìš°íŒ…"""
        step = state.get("step")

        if state.get("error"):
            return "error_handler"

        if step == "index":
            return "vector_indexer"
        elif step == "query":
            return "query_agent"
        elif step == "summarize":
            return "summary_agent"
        else:
            return "complete_processing"

    async def error_handler(self, state: AnalysisState) -> AnalysisState:
        """ì—ëŸ¬ ì²˜ë¦¬"""
        error = state.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")
        retry_count = state.get("retry_count", 0)

        print(f"ì—ëŸ¬ ë°œìƒ: {error}, ì¬ì‹œë„ íšŸìˆ˜: {retry_count}")

        if retry_count < 3:
            # ì¬ì‹œë„ ë¡œì§
            state["retry_count"] = retry_count + 1
            state["error"] = None
            state["processing_status"] = "retrying"
        else:
            state["processing_status"] = "failed"

        return state

    async def complete_processing(self, state: AnalysisState) -> AnalysisState:
        """ì²˜ë¦¬ ì™„ë£Œ"""
        state["processing_status"] = "completed"
        return state

    # í—¬í¼ ë©”ì„œë“œë“¤
    def _validate_file(self, file_path: str) -> bool:
        """íŒŒì¼ ê²€ì¦ í—¬í¼ ë©”ì†Œë“œ"""
        return Path(file_path).exists() and Path(file_path).suffix.lower() == ".pdf"

    async def _get_vectorstore_stats(self) -> dict:
        """ë²¡í„°ìŠ¤í† ì–´ í†µê³„ ì¡°íšŒ"""
        try:
            # Chroma vectorstoreì—ì„œ ë¬¸ì„œ ê°œìˆ˜ ì¡°íšŒ
            collection = self.vectorstore._collection
            return {
                "total_vectors": collection.count(),
                "collection_name": self.vectorstore._collection.name,
            }
        except Exception as e:
            return {"error": str(e)}

    async def _search_relevant_chunks(self, document_id: str, query: str) -> List[str]:
        # ì‹¤ì œ RAG ê²€ìƒ‰ ë¡œì§
        return ["ê´€ë ¨ ì²­í¬1", "ê´€ë ¨ ì²­í¬2"]

    def _calculate_confidence(self, query: str, chunks: List[str]) -> float:
        # ì‹ ë¢°ë„ ê³„ì‚° ë¡œì§
        return 0.85

    def _get_auto_summary_prompt(self) -> str:
        return """
        ë‹¹ì‹ ì€ ê¸ˆìœµ ë¬¸ì„œ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
        ê³µì‹œ ë¬¸ì„œë¥¼ ì½ê³  íˆ¬ììê°€ ì•Œì•„ì•¼ í•  í•µì‹¬ ì‚¬í•­ë“¤ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.
        
        ë¬¸ì„œ ë‚´ìš©: {content}
        
        ë‹¤ìŒ êµ¬ì¡°ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:
        ## ğŸ“Š ì¬ë¬´ í•˜ì´ë¼ì´íŠ¸
        ## ğŸ¢ ì£¼ìš” ì‚¬ì—… í˜„í™©  
        ## âš ï¸ ìœ„í—˜ ìš”ì¸
        ## ğŸ“ˆ í–¥í›„ ì „ë§
        ## ğŸ’¡ íˆ¬ì í¬ì¸íŠ¸
        """

    def _parse_summary_response(self, response: str) -> Dict[str, Any]:
        # ì‘ë‹µ íŒŒì‹± ë¡œì§
        return {
            "financial_highlights": "ì¬ë¬´ í•˜ì´ë¼ì´íŠ¸...",
            "business_status": "ì‚¬ì—… í˜„í™©...",
            "risk_factors": "ìœ„í—˜ ìš”ì¸...",
            "future_outlook": "í–¥í›„ ì „ë§...",
            "investment_points": "íˆ¬ì í¬ì¸íŠ¸...",
        }
